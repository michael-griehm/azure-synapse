{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Datalake Access Key configuration\n",
    "keyvault_name = \"cryptoanalyticssynapse\"\n",
    "secret_name = \"cryptoanalyticslake\"\n",
    "linked_service_name = \"keyvault\"\n",
    "\n",
    "adls_token = mssparkutils.credentials.getSecret(keyvault_name, secret_name, linked_service_name)\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.key.cryptoanalyticslake.dfs.core.windows.net\",adls_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Day Month Year\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "today = datetime.utcnow()\n",
    "year = today.year\n",
    "month = today.month\n",
    "day = today.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive data load for all files from a day from every partition in the Event Hub Namespace\n",
    "sourcefolderpath = f\"abfss://crypto-quotes@cryptoanalyticslake.dfs.core.windows.net/ehns-quote-streams/eh-crypto-stream/*/{year}/{month:0>2d}/{day:0>2d}\"\n",
    "\n",
    "print(sourcefolderpath)\n",
    "\n",
    "df = spark.read.option(\"recursiveFileLookup\",\"true\").option(\"header\",\"true\").format(\"avro\").load(sourcefolderpath)\n",
    "\n",
    "display(df)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the Body field from Binary to JSON \n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StringType, DoubleType, StructType, StructField\n",
    "\n",
    "sourceSchema = StructType([\n",
    "        StructField(\"Symbol\", StringType(), False),\n",
    "        StructField(\"Price\", DoubleType(), True),\n",
    "        StructField(\"PriceTimeStamp\", StringType(), True)])\n",
    "\n",
    "df = df.withColumn(\"StringBody\", col(\"Body\").cast(\"string\"))\n",
    "jsonOptions = {\"dateFormat\" : \"yyyy-MM-dd HH:mm:ss.SSS\"}\n",
    "df = df.withColumn(\"JsonBody\", from_json(df.StringBody, sourceSchema, jsonOptions))\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattent he Body JSON field into columns of the DataFrame\n",
    "for c in df.schema[\"JsonBody\"].dataType:\n",
    "    df = df.withColumn(c.name, col(\"JsonBody.\" + c.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 0 priced assets\n",
    "df = df.filter(\"Price > 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data\n",
    "df = df.sort(\"Symbol\", \"PriceTimeStamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the meaningful columns for the export to Bronze data zone\n",
    "exportDF = df.select(\"Symbol\", \"Price\", \"PriceTimeStamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the partquet file in the bronze crypto data zone\n",
    "destinationfolderpath = f\"abfss://crypto-bronze@cryptoanalyticslake.dfs.core.windows.net/quotes-by-day/{year}/{month:0>2d}/{day:0>2d}\"\n",
    "\n",
    "print(destinationfolderpath)\n",
    "\n",
    "exportDF.write.mode(\"overwrite\").parquet(destinationfolderpath)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
