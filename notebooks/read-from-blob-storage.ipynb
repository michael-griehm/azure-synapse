{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "# Azure storage access info \n",
    "\n",
    "blob_account_name = 'cryptoanalyticssynapse' # replace with your blob name \n",
    "blob_container_name = 'synapse-root' # replace with your container name \n",
    "blob_relative_path = 'snowflake/2022/04/17' # replace with your relative folder path \n",
    "linked_service_name = 'BlobSasToken' # replace with your linked service name using Account Key credential method\n",
    "\n",
    "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name) \n",
    "\n",
    "print(blob_sas_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow SPARK to access from Blob remotely \n",
    "\n",
    "wasb_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path) \n",
    "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token) \n",
    "print('Remote blob path: ' + wasb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(wasb_path)\n",
    "\n",
    "display(df)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.count())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
